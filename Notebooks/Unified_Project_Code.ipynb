{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS7lxX6Ettyu"
      },
      "source": [
        "# Apple Leaf Disease Recognition Using Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-oBEkW7t2Bt"
      },
      "source": [
        "\n",
        "\n",
        "*   [Baseline Model](https://www.google.com/url?q=https%3A%2F%2Fwww.kaggle.com%2Fcode%2Fchanchal24%2Fplant-disease-recognition-using-dl%2Fnotebook)\n",
        "*   [Dataset](https://www.google.com/url?q=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Frashikrahmanpritom%2Fplant-disease-recognition-dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SvKcR9UNr0h"
      },
      "outputs": [],
      "source": [
        "pip install torch torchvision tensorflow keras numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osn3hlIEuH7m"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGXfviapw8Fd"
      },
      "outputs": [],
      "source": [
        "# PyTorch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# TensorFlow/Keras Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout,\n",
        "    BatchNormalization, ReLU, GlobalAveragePooling2D, ZeroPadding2D\n",
        ")\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "\n",
        "# General Utilities\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZwuASIztn_o"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DFdksNkx3Lg"
      },
      "outputs": [],
      "source": [
        "# Function to count the total files in a directory\n",
        "def total_files(directory):\n",
        "    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtbTOWlJuNC1"
      },
      "outputs": [],
      "source": [
        "# Root directories for training, validation, and testing\n",
        "#Charbel's path\n",
        "train_dir = \"/content/drive/MyDrive/AML_Dataset/Train/Train\"\n",
        "val_dir = \"/content/drive/MyDrive/AML_Dataset/Validation/Validation\"\n",
        "test_dir = \"/content/drive/MyDrive/AML_Dataset/Test/Test\"\n",
        "\n",
        "#Nakhla's path\n",
        "# train_dir=\"/content/drive/MyDrive/AML_Dataset/Train/Train\"\n",
        "# val_dir = \"/content/drive/MyDrive/AML_Dataset/Validation/Validation\"\n",
        "# test_dir = \"/content/drive/MyDrive/AML_Dataset/Test/Test\"\n",
        "# Target image size and batch size\n",
        "image_size = (225, 225)\n",
        "batch_size = 32\n",
        "\n",
        "# Class-specific paths\n",
        "train_files_healthy = f\"{train_dir}/Healthy\"\n",
        "train_files_powdery = f\"{train_dir}/Powdery\"\n",
        "train_files_rust = f\"{train_dir}/Rust\"\n",
        "\n",
        "test_files_healthy = f\"{test_dir}/Healthy\"\n",
        "test_files_powdery = f\"{test_dir}/Powdery\"\n",
        "test_files_rust = f\"{test_dir}/Rust\"\n",
        "\n",
        "valid_files_healthy = f\"{val_dir}/Healthy\"\n",
        "valid_files_powdery = f\"{val_dir}/Powdery\"\n",
        "valid_files_rust = f\"{val_dir}/Rust\"\n",
        "\n",
        "# Print file counts for each class\n",
        "print(\"Number of healthy leaf images in training set\", total_files(train_files_healthy))\n",
        "print(\"Number of powder leaf images in training set\", total_files(train_files_powdery))\n",
        "print(\"Number of rusty leaf images in training set\", total_files(train_files_rust))\n",
        "\n",
        "print(\"========================================================\")\n",
        "\n",
        "print(\"Number of healthy leaf images in test set\", total_files(test_files_healthy))\n",
        "print(\"Number of powder leaf images in test set\", total_files(test_files_powdery))\n",
        "print(\"Number of rusty leaf images in test set\", total_files(test_files_rust))\n",
        "\n",
        "print(\"========================================================\")\n",
        "\n",
        "print(\"Number of healthy leaf images in validation set\", total_files(valid_files_healthy))\n",
        "print(\"Number of powder leaf images in validation set\", total_files(valid_files_powdery))\n",
        "print(\"Number of rusty leaf images in validation set\", total_files(valid_files_rust))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNO_7M-_udnV"
      },
      "source": [
        "# Visualizing Images for Different Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L42DIlnkyZEa"
      },
      "outputs": [],
      "source": [
        "# Helper function to display an image from a given directory and filename\n",
        "def display_image(directory, filename, width=500):\n",
        "    image_path = os.path.join(directory, filename)\n",
        "    with open(image_path, 'rb') as f:\n",
        "        display.display(display.Image(data=f.read(), width=width))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qfILD78ugcx"
      },
      "source": [
        "## Healthy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSUedCNHypaC"
      },
      "outputs": [],
      "source": [
        "print(\"Displaying a sample image from the Healthy class (Train):\")\n",
        "display_image(train_files_healthy, '8ce77048e12f3dd4.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC_epHu-uiMy"
      },
      "source": [
        "## Rust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezGJIJvIukH3"
      },
      "outputs": [],
      "source": [
        "print(\"Displaying a sample image from the Rust class (Train):\")\n",
        "display_image(train_files_rust, '80f09587dfc7988e.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrZLAK8suk2Z"
      },
      "source": [
        "## Powdery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay_UTJUZunll"
      },
      "outputs": [],
      "source": [
        "print(\"Displaying a sample image from the Powdery class (Train):\")\n",
        "display_image(train_files_powdery, '8b7569be32192c1e.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc7VjsntPoS6"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3xLpAfIPrU6"
      },
      "outputs": [],
      "source": [
        "augmented_train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'        # Fill pixels after transformations\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzDJsruRQEkx"
      },
      "outputs": [],
      "source": [
        "# Now, we will create the directory to save the train augmented data\n",
        "# Directory to store augmented images\n",
        "#Charbel's Path\n",
        "# augmented_train_dir = \"/content/drive/MyDrive/Advanced ML/Final Project/AML_Dataset/Augmented_Train\"\n",
        "\n",
        "#Nakhla's path\n",
        "augmented_train_dir= \"/content/drive/MyDrive/AML_Dataset/Augmented_Train\"\n",
        "\n",
        "# Create directories for each class\n",
        "for class_dir in ['Healthy', 'Powdery', 'Rust']:\n",
        "    class_path = os.path.join(augmented_train_dir, class_dir)\n",
        "    if os.path.exists(class_path):\n",
        "        shutil.rmtree(class_path)  # Clear existing files if directory exists\n",
        "    os.makedirs(class_path)  # Create fresh directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSE7ZPoTQXLl"
      },
      "outputs": [],
      "source": [
        "# Augmented data generator\n",
        "augmented_generator = augmented_train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=1,  # Save one image at a time\n",
        "    class_mode='categorical',\n",
        "    shuffle=True  # Shuffle to get varied batches\n",
        ")\n",
        "\n",
        "# Generate and save augmented images in class-specific folders\n",
        "num_augmented_images = 1000  # Total number of augmented images to create\n",
        "class_indices = augmented_generator.class_indices  # Mapping of classes to indices\n",
        "reverse_class_indices = {v: k for k, v in class_indices.items()}  # Reverse mapping\n",
        "\n",
        "for i in range(num_augmented_images):\n",
        "    batch = next(augmented_generator)  # Get the next batch (image, label)\n",
        "    image, label = batch[0], batch[1]\n",
        "\n",
        "    # Find the class label from the one-hot encoded label\n",
        "    class_index = label.argmax()  # Get index of the class\n",
        "    class_name = reverse_class_indices[class_index]  # Get the class name\n",
        "\n",
        "    # Save the image in the corresponding class folder\n",
        "    save_dir = os.path.join(augmented_train_dir, class_name)\n",
        "    augmented_generator.save_to_dir = save_dir  # Dynamically update save directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3_WitEXRBU5"
      },
      "outputs": [],
      "source": [
        "# Function to count files in a directory\n",
        "def total_files(directory):\n",
        "    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
        "\n",
        "print(\"Updated counts for augmented training dataset:\")\n",
        "for class_dir in ['Healthy', 'Powdery', 'Rust']:\n",
        "    class_path = os.path.join(augmented_train_dir, class_dir)\n",
        "    print(f\"Number of images in {class_dir} class: {total_files(class_path)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBedkkKZdsaP"
      },
      "source": [
        "## Adding the Augmented train data, to the original one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_B_cLj5dxgz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create the combined directory\n",
        "if os.path.exists(combined_train_dir):\n",
        "    shutil.rmtree(combined_train_dir)  # Clear existing directory\n",
        "os.makedirs(combined_train_dir)\n",
        "\n",
        "# Copy images from both original and augmented datasets into combined directory\n",
        "for class_name in ['Healthy', 'Powdery', 'Rust']:\n",
        "    # Create class subdirectories in the combined directory\n",
        "    class_combined_dir = os.path.join(combined_train_dir, class_name)\n",
        "    os.makedirs(class_combined_dir)\n",
        "\n",
        "    # Copy images from the original dataset\n",
        "    original_class_dir = os.path.join(original_train_dir, class_name)\n",
        "    for file_name in os.listdir(original_class_dir):\n",
        "        src_path = os.path.join(original_class_dir, file_name)\n",
        "        dst_path = os.path.join(class_combined_dir, file_name)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "    # Copy images from the augmented dataset\n",
        "    augmented_class_dir = os.path.join(augmented_train_dir, class_name)\n",
        "    for file_name in os.listdir(augmented_class_dir):\n",
        "        src_path = os.path.join(augmented_class_dir, file_name)\n",
        "        dst_path = os.path.join(class_combined_dir, file_name)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"Combined dataset created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLYJx3YDd6jS"
      },
      "outputs": [],
      "source": [
        "# Function to count total files in a directory\n",
        "def total_files(directory):\n",
        "    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
        "\n",
        "# Print counts for each class in the combined dataset\n",
        "print(\"Image counts in Combined_Train:\")\n",
        "for class_name in ['Healthy', 'Powdery', 'Rust']:\n",
        "    class_path = os.path.join(combined_train_dir, class_name)\n",
        "    print(f\"{class_name}: {total_files(class_path)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lmR_KzTe8Dz"
      },
      "source": [
        "Data Successfully augmented!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgIcQEl5EFtO"
      },
      "source": [
        "# Preprocessing For Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbM4hg1GEps3"
      },
      "source": [
        "Checking for cuda and loading to gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYEP52szEPC2"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU found.\")\n",
        "\n",
        "# Assign device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpDhV26wEwO1"
      },
      "source": [
        "Preprocessing images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRgOOF4aEUVe"
      },
      "outputs": [],
      "source": [
        "trans=transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "#Original Dataset\n",
        "train=datasets.ImageFolder(\"/content/drive/MyDrive/AML_Dataset/Train/Train\",transform=trans)\n",
        "validation=datasets.ImageFolder(\"/content/drive/MyDrive/AML_Dataset/Validation/Validation\",transform=trans)\n",
        "test=datasets.ImageFolder(\"/content/drive/MyDrive/AML_Dataset/Test/Test\",transform=trans)\n",
        "\n",
        "#Augmented Dataset\n",
        "\n",
        "class_names=train.classes\n",
        "\n",
        "all_data = [data for data in train]\n",
        "validationData=[data for data in validation]\n",
        "testData=[data for data in test]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjiWPLIUE0a7"
      },
      "source": [
        "Pytorch implementation for dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_2sLqvIEhav"
      },
      "outputs": [],
      "source": [
        "batch=64\n",
        "train_loader = DataLoader(train, batch_size=batch, num_workers=4, pin_memory=True, shuffle=True)\n",
        "validation_loader = DataLoader(validation, batch_size=batch, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test, batch_size=batch, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lv09l8zuv1t"
      },
      "source": [
        "# Preprocessing For TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bSHM4RJP5xL"
      },
      "outputs": [],
      "source": [
        "# Directories\n",
        "# #Charbel's path\n",
        "original_train_dir = \"/content/drive/MyDrive/Advanced ML/Final Project/AML_Dataset/Train/Train\"\n",
        "augmented_train_dir = \"/content/drive/MyDrive/Advanced ML/Final Project/AML_Dataset/Augmented_Train\"\n",
        "combined_train_dir = \"/content/drive/MyDrive/Advanced ML/Final Project/AML_Dataset/Combined_Train\"\n",
        "\n",
        "#Nakhla's path\n",
        "# original_train_dir = \"/content/drive/MyDrive/AML_Dataset/Train/Train\"\n",
        "# augmented_train_dir = \"/content/drive/MyDrive/AML_Dataset/Augmented_Train\"\n",
        "# combined_train_dir = \"/content/drive/MyDrive/AML_Dataset/Combined_Train\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIaL6nkQOJd9"
      },
      "source": [
        "Unaugmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4SkZLakbpZk"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for the unaugmented data\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPp5vpfSOL9x"
      },
      "source": [
        "Combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt01ZmcJcA94"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for combined dataset\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    combined_train_dir,\n",
        "    # target_size=image_size,\n",
        "    target_size=(256, 256) ,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxdgAkFRONws"
      },
      "source": [
        "Validation and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOJv2hzYuxH7"
      },
      "outputs": [],
      "source": [
        "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDKB5vOexDNs"
      },
      "outputs": [],
      "source": [
        "# Creating generators\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    # target_size=image_size,\n",
        "    target_size=(256, 256) ,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    # target_size=image_size,\n",
        "    target_size=(256, 256) ,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6102KMoChhB9"
      },
      "source": [
        "Inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPxxWe-YhgYW"
      },
      "outputs": [],
      "source": [
        "# Define image size and batch size\n",
        "img_size = (256, 256)\n",
        "batch_size = 32\n",
        "\n",
        "# Data preprocessing and augmentation using ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create data generators for loading the data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    combined_train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Rvu4nb0cjM"
      },
      "source": [
        "# Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diEw5_wJ0eOB"
      },
      "outputs": [],
      "source": [
        "model_baseline = Sequential()\n",
        "# Input layer\n",
        "model_baseline.add(Input(shape=(225, 225, 3)))\n",
        "\n",
        "# First Conv Block\n",
        "model_baseline.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model_baseline.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Conv Block\n",
        "model_baseline.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model_baseline.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten and Dense Layers\n",
        "model_baseline.add(Flatten())\n",
        "model_baseline.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "\n",
        "# Output Layer\n",
        "model_baseline.add(Dense(3, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvy9Ryde20Zf"
      },
      "source": [
        "# Updated Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNWbDjWy23Jk"
      },
      "outputs": [],
      "source": [
        "model_baseline2 = Sequential()\n",
        "\n",
        "# First Conv Block\n",
        "model_baseline2.add(Conv2D(32, (3, 3), input_shape=(225, 225, 3), activation='relu'))\n",
        "model_baseline2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Conv Block\n",
        "model_baseline2.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model_baseline2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Third Conv Block\n",
        "model_baseline2.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model_baseline2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Fourth Conv Block\n",
        "model_baseline2.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model_baseline2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Fifth Conv Block\n",
        "model_baseline2.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model_baseline2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten Layer\n",
        "model_baseline2.add(Flatten())\n",
        "\n",
        "# Fully Connected Layers\n",
        "model_baseline2.add(Dense(256, activation='relu'))\n",
        "model_baseline2.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model_baseline2.add(Dense(3, activation='softmax'))  # 3 classes: Healthy, Powdery, Rust"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKgcjz8x3tEV"
      },
      "source": [
        "# CNN 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJk-Hp5j3uRE"
      },
      "outputs": [],
      "source": [
        "model_cnn = Sequential()\n",
        "\n",
        "# First Conv Block\n",
        "model_cnn.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(225, 225, 3)))\n",
        "model_cnn.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model_cnn.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\n",
        "# Second Conv Block\n",
        "model_cnn.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model_cnn.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model_cnn.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\n",
        "# Third Conv Block\n",
        "model_cnn.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model_cnn.add(Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model_cnn.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "\n",
        "# Fourth Conv Block\n",
        "model_cnn.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model_cnn.add(Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "\n",
        "# Fifth Conv Block\n",
        "model_cnn.add(Conv2D(512, (5, 5), activation=\"relu\", padding=\"same\"))\n",
        "model_cnn.add(Conv2D(512, (5, 5), activation=\"relu\", padding=\"same\"))\n",
        "\n",
        "# Flatten and Dense Layers\n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(1568, activation=\"relu\"))\n",
        "model_cnn.add(Dropout(0.5))  # Added dropout for regularization\n",
        "model_cnn.add(Dense(3, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25sB9cgvLfOW"
      },
      "source": [
        "# Pytorch CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AkpQ_fhAGpO"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNet,self).__init__()\n",
        "    self.network=nn.Sequential(\n",
        "        nn.ZeroPad2d(3),\n",
        "        nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=2),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=2),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(576,3),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x=self.network(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vfdhug8Oz1_"
      },
      "outputs": [],
      "source": [
        "model_cnn2=NeuralNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchviz\n"
      ],
      "metadata": {
        "id": "KQnB-X4vb4HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5wUS3IUJCLk"
      },
      "source": [
        "# ResNet 34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EmP0FcXLC_v"
      },
      "outputs": [],
      "source": [
        "# Define the residual block for ResNet\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1): # Changed _init_ to __init__\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet-34 model\n",
        "class ResNet34(nn.Module):\n",
        "    def __init__(self, num_classes=38): # Changed _init_ to __init__\n",
        "        super(ResNet34, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Define the stages\n",
        "        self.layer1 = self._make_layer(64, 64, 3)\n",
        "        self.layer2 = self._make_layer(64, 128, 4, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 6, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 3, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RBwx82MmMAAA"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = ResNet34(num_classes=len(class_names))\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Load data\n",
        "train_loader = DataLoader(train, batch_size=64, shuffle=True, num_workers=4)\n",
        "validation_loader = DataLoader(validation, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize lists to store loss and accuracy values\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "epochs = 7\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_val_loss / len(validation_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIYR3IVtYBnH"
      },
      "outputs": [],
      "source": [
        "# Plotting loss and accuracy curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curves')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Accuracy Curves')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bhI350F4QzU"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT0MSKha4SGU"
      },
      "outputs": [],
      "source": [
        "model_rnn = Sequential()\n",
        "\n",
        "# Reshape the input to (time steps, features) for RNN layers\n",
        "model_rnn.add(Reshape((225, 225*3), input_shape=(225, 225, 3)))  # Adjusted to match (225, 225, 3)\n",
        "\n",
        "# Stacked SimpleRNN layers\n",
        "model_rnn.add(SimpleRNN(128, return_sequences=True))\n",
        "model_rnn.add(SimpleRNN(128, return_sequences=True))\n",
        "model_rnn.add(SimpleRNN(128))\n",
        "\n",
        "# Fully Connected Layers\n",
        "model_rnn.add(Dense(1568, activation=\"relu\"))\n",
        "model_rnn.add(Dropout(0.5))  # Regularization to prevent overfitting\n",
        "\n",
        "# Output Layer\n",
        "model_rnn.add(Dense(3, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO537FWW4njS"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyYUxBeL4o0S"
      },
      "outputs": [],
      "source": [
        "model_lstm = Sequential()\n",
        "\n",
        "# Reshape the input to (time steps, features) for LSTM layers\n",
        "model_lstm.add(Reshape((225, 225*3), input_shape=(225, 225, 3)))  # Adjusted to match (225, 225, 3)\n",
        "\n",
        "# Add LSTM layers\n",
        "model_lstm.add(LSTM(128, return_sequences=True))\n",
        "model_lstm.add(LSTM(128, return_sequences=True))\n",
        "model_lstm.add(LSTM(128))\n",
        "\n",
        "# Fully Connected Layers\n",
        "model_lstm.add(Dense(1568, activation=\"relu\"))\n",
        "model_lstm.add(Dropout(0.5))  # Regularization to prevent overfitting\n",
        "\n",
        "# Output Layer\n",
        "model_lstm.add(Dense(3, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5hYUXKJIok7"
      },
      "source": [
        "# Inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhE1vMUHIqbR"
      },
      "outputs": [],
      "source": [
        "# Load the InceptionV3 model pre-trained on ImageNet without the top layer (for transfer learning)\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:  # Freeze all but the top 30 layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom classifier with more capacity\n",
        "model_inception = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='relu'),  # Additional dense layer\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(3, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2a_TtUMzqsJ"
      },
      "source": [
        "# Tensorflow Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3AC-t--GV5s"
      },
      "outputs": [],
      "source": [
        "# default_loss = 'categorical_crossentropy'\n",
        "default_loss = 'categorical_crossentropy'\n",
        "default_metrics = ['accuracy']\n",
        "default_learning_rate = 1e-5\n",
        "\n",
        "def compile_model(model, loss=default_loss, metrics=default_metrics, learning_rate=default_learning_rate):\n",
        "    optimizer_instance = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer_instance, loss=loss, metrics=metrics)\n",
        "    print(f\"Model compiled with Adam optimizer (learning_rate={learning_rate}), loss={loss}, metrics={metrics}\")\n",
        "    model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1yNEw-JGXWl"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_data, validation_data, epochs=5, batch_size=16):\n",
        "    print(f\"Starting training for {epochs} epochs with batch size {batch_size}...\")\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_data,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    print(\"Training complete!\")\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9TiwXrzGYfu"
      },
      "outputs": [],
      "source": [
        "compile_model(model_inception)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1icGP2xGYkN"
      },
      "outputs": [],
      "source": [
        "# # # Train the model\n",
        "# history = train_model(\n",
        "#     model_inception,\n",
        "#     train_data=train_generator,\n",
        "#     validation_data=validation_generator,\n",
        "#     epochs=15,\n",
        "#     batch_size=16,\n",
        "# )\n",
        "\n",
        "#Inception\n",
        "# Train the model\n",
        "epochs = 10\n",
        "history = model_inception.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er7j-A9aKUu2"
      },
      "outputs": [],
      "source": [
        "model_inception.save(\"inception_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhR8tE4hGTGm"
      },
      "source": [
        "#Pytorch Modelling & Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDEyX-g1GrWh"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_cnn2.parameters(), lr=0.0003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eBEc0n0GeEl"
      },
      "outputs": [],
      "source": [
        "def train(train_loader):\n",
        "    model_cnn2.train()\n",
        "    loss_per_epoch = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        outputs = model_cnn2(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        loss_per_epoch += loss.item()\n",
        "        preds = torch.argmax(outputs, dim=1)  # Get predicted labels\n",
        "        correct += (preds == labels).sum().item()  # Count correct predictions\n",
        "        total += labels.size(0)  # Update total count\n",
        "\n",
        "    train_accuracy = correct / total  # Compute accuracy\n",
        "    return loss_per_epoch, train_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4CnWIOOJli9"
      },
      "outputs": [],
      "source": [
        "def validate(val_loader):\n",
        "    model_cnn2.eval()\n",
        "    loss_per_epoch = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model_cnn2(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss_per_epoch += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct / total  # Compute accuracy\n",
        "    return loss_per_epoch, val_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnGfZz7wBZyW"
      },
      "outputs": [],
      "source": [
        "epochs=10\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    # Training\n",
        "    loss_train, acc_train = train(train_loader)\n",
        "    # Validation\n",
        "    loss_val, acc_val = validate(validation_loader)\n",
        "\n",
        "    # Store results\n",
        "    train_loss.append(loss_train)\n",
        "    val_loss.append(loss_val)\n",
        "    train_acc.append(acc_train)\n",
        "    val_acc.append(acc_val)\n",
        "\n",
        "    print(f\"Epoch {i+1}/{epochs} -> Train Loss: {loss_train:.4f}, Train Acc: {acc_train:.4f}, \"\n",
        "          f\"Val Loss: {loss_val:.4f}, Val Acc: {acc_val:.4f}\")\n",
        "plt.plot(train_loss,label=\"training loss\")\n",
        "plt.plot(val_loss,label=\"validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_cnn2.state_dict(), \"model_cnn.pth\")\n",
        "print(\"Model weights saved successfully.\")"
      ],
      "metadata": {
        "id": "guoTR1F5NhfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xxVNiWrPJiE"
      },
      "outputs": [],
      "source": [
        "# def test(dataloader, model, loss_fn):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     num_batches = len(dataloader)\n",
        "#     model.eval()\n",
        "#     test_loss, correct = 0, 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for X, y in dataloader:\n",
        "#             X, y = X.float().to(device), y.to(device)\n",
        "#             pred = model(X)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "#             preds = torch.argmax(pred, dim=1)\n",
        "#             correct += (preds == y).sum().item()\n",
        "\n",
        "#     test_loss /= num_batches\n",
        "#     test_accuracy = correct / size  # Compute accuracy\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*test_accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "#     return test_loss, test_accuracy\n",
        "\n",
        "# test(test_loader,model_cnn2,criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnF-W4tbp04c"
      },
      "source": [
        "# Tensorflow Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgTQcl590xos"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "sns.set_context(\"poster\")\n",
        "\n",
        "def plot_accuracy(history, figsize=(10, 6), dpi=100):\n",
        "    # Use plt.figure instead of just figure\n",
        "    plt.figure(figsize=figsize, dpi=dpi)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "    plt.title('Model Accuracy', fontsize=16)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss(history, figsize=(10, 6), dpi=100):\n",
        "    # Use plt.figure instead of just figure\n",
        "    plt.figure(figsize=figsize, dpi=dpi)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.title('Model Loss', fontsize=16)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYnJMxK91PIc"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy\n",
        "plot_accuracy(history, figsize=(12, 8), dpi=100)\n",
        "\n",
        "# Plot loss\n",
        "plot_loss(history, figsize=(12, 8), dpi=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMmDP2B6IKy8"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history[\"train_loss\"], label=\"Training Loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss Over Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history[\"train_acc\"], label=\"Training Accuracy\")\n",
        "    plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy Over Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_AFlrxnFrfl"
      },
      "source": [
        "# Model Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQzxcA_6Y_N6"
      },
      "outputs": [],
      "source": [
        "# model_cnn= load_model('model_cnn.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KC7AOzU1mUD"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path, target_size=(256, 256)):  # Change target size to (256, 256)\n",
        "    \"\"\"\n",
        "    Preprocesses an image for prediction.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the image with the target size\n",
        "        img = load_img(image_path, target_size=target_size)  # Resize to (256, 256)\n",
        "        # Convert image to numpy array\n",
        "        x = img_to_array(img)\n",
        "        # Normalize pixel values to [0, 1]\n",
        "        x = x.astype('float32') / 255.0\n",
        "        # Add a batch dimension (required for prediction)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        return x\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image at {image_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cGA6GFO1ru0"
      },
      "outputs": [],
      "source": [
        "# Usage\n",
        "# image_path = '/content/drive/MyDrive/Advanced ML/Final Project/AML_Dataset/Test/Test/Rust/82f49a4a7b9585f1.jpg'\n",
        "\n",
        "#Nakhla's path\n",
        "image_path = '/content/drive/MyDrive/AML_Dataset/Test/Test/Rust/82f49a4a7b9585f1.jpg'\n",
        "\n",
        "# Test the function again\n",
        "x = preprocess_image(image_path)\n",
        "\n",
        "if x is not None:\n",
        "    print(\"Image preprocessed successfully!\")\n",
        "    print(f\"Shape of preprocessed image: {x.shape}\")\n",
        "else:\n",
        "    print(\"Failed to preprocess image.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbOSxFKj1vE7"
      },
      "outputs": [],
      "source": [
        "# Predict class probabilities for the preprocessed image\n",
        "predictions = model_cnn2.predict(x)\n",
        "\n",
        "# Display raw predictions (probabilities)\n",
        "print(\"Raw predictions (class probabilities):\", predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J5Hf7Ai2Fqg"
      },
      "outputs": [],
      "source": [
        "# Retrieve and reverse class indices\n",
        "labels = train_generator.class_indices\n",
        "labels = {v: k for k, v in labels.items()}\n",
        "print(\"Class labels mapping:\", labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDkez7vN2L5f"
      },
      "outputs": [],
      "source": [
        "# Find the predicted class\n",
        "predicted_index = np.argmax(predictions[0])\n",
        "predicted_label = labels[predicted_index]\n",
        "\n",
        "# Display the predicted label\n",
        "print(f\"Predicted Label: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "Ukj3b2uoP5lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor, Resize, Normalize, Compose\n",
        "from PIL import Image\n",
        "\n",
        "# Update preprocessing for PyTorch\n",
        "def preprocess_image(image_path, target_size=(256, 256)):\n",
        "    original_img = Image.open(image_path).convert(\"RGB\")  # Load and convert to RGB\n",
        "    transform = Compose([\n",
        "        Resize(target_size),  # Resize the image\n",
        "        ToTensor(),           # Convert to tensor\n",
        "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize (ImageNet stats)\n",
        "    ])\n",
        "    preprocessed_img = transform(original_img).unsqueeze(0)  # Add batch dimension\n",
        "    return preprocessed_img, original_img\n",
        "\n",
        "def predict_and_display(image_path, model, class_names):\n",
        "    # Preprocess the image\n",
        "    preprocessed_img, original_img = preprocess_image(image_path)\n",
        "\n",
        "    # Move the image and model to the device\n",
        "    preprocessed_img = preprocessed_img.to(device)\n",
        "    model = model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(preprocessed_img)  # Forward pass\n",
        "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)  # Softmax for probabilities\n",
        "        predicted_class_index = torch.argmax(probabilities).item()\n",
        "        predicted_class_name = class_names[predicted_class_index]\n",
        "        confidence_score = probabilities[predicted_class_index].item()\n",
        "\n",
        "    # Display the image, predicted class, and confidence\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(original_img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Predicted: {predicted_class_name}\\nConfidence: {confidence_score:.2f}\", fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    # Print confidence scores for each class\n",
        "    print(\"Confidence scores for each class:\")\n",
        "    for i, score in enumerate(probabilities):\n",
        "        print(f\"{class_names[i]}: {score:.2f}\")"
      ],
      "metadata": {
        "id": "rrgn8qtlP7AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\"Healthy\", \"Powdery\", \"Rust\"]  # Update with your actual class names"
      ],
      "metadata": {
        "id": "bq5exe_MV8G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Healthy"
      ],
      "metadata": {
        "id": "5ijLlyo2UUeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with an image\n",
        "image_path = \"/content/drive/MyDrive/AML_Dataset/Test/Test/Healthy/8ddd5ec1c0de38c4.jpg\"\n",
        "predict_and_display(image_path, model_cnn2, class_names)"
      ],
      "metadata": {
        "id": "RYbfwSBAUSwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rust"
      ],
      "metadata": {
        "id": "cOfU9CHuUZPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with an image\n",
        "image_path = \"/content/drive/MyDrive/AML_Dataset/Test/Test/Rust/90bb8dcc6f0c58b9.jpg\"\n",
        "predict_and_display(image_path, model_cnn2, class_names)"
      ],
      "metadata": {
        "id": "xf8IU2UwUakQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Rc7VjsntPoS6",
        "08Rvu4nb0cjM",
        "Lvy9Ryde20Zf",
        "qKgcjz8x3tEV",
        "J5wUS3IUJCLk",
        "2bhI350F4QzU",
        "CO537FWW4njS",
        "Z5hYUXKJIok7"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}