<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Plant Disease Detection</title>

  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark static-top">
    <div class="container">
      <a class="navbar-brand" href="../home.html">Practical Machine Deep Learning</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="../home.html">Home
              <span class="sr-only">(current)</span>
            </a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../about.html">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../contact.html">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h1 class="mt-5">Plant Disease Detection</h1>
        <ul class="list-unstyled">
          <li>Youssef Nakhla 900201430</li>
          <li>Charbel El Fakhry 900214262</li>
        </ul>
        <div class="img-container" align="center">
          <img src="resources/images/Project-title.png" class="img-fluid text-center">
        </div>
      </div>
    </div>

    <!-- Problem Statement -->
    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Problem Statement</h2>
        <p>
          Egypt relies heavily on agriculture as a key sector supporting its economy and food security, with a large portion of the population engaged in farming activities. The aim of this project is to develop a machine learning algorithm using deep learning architectures that can accurately recognize and classify diseases affecting apple leaves to help support farmers by enabling early detection, reducing crop losses.
        </p>
      </div>
    </div>

    <!-- Dataset -->
    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Dataset</h2>
        <p>
          This dataset contains 1530 images of apple tree leaves. Moreover, the data is labeled under 3 different labels: Healthy, Powdery, and Rust. It is already split into training, testing, and validation subsets. The class distributions are:
        </p>
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Subset</th>
              <th>Healthy</th>
              <th>Powdery</th>
              <th>Rust</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Training</td>
              <td>458</td>
              <td>430</td>
              <td>434</td>
            </tr>
            <tr>
              <td>Testing</td>
              <td>50</td>
              <td>50</td>
              <td>50</td>
            </tr>
            <tr>
              <td>Validation</td>
              <td>20</td>
              <td>20</td>
              <td>20</td>
            </tr>
          </tbody>
        </table>
        <p>
          <a href="https://www.kaggle.com/datasets/rashikrahmanpritom/plant-disease-recognition-dataset" target="_blank">Dataset URL</a>
        </p>
      </div>
    </div>

    <!-- Input/Output Examples -->
    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Input/Output Examples</h2>
        <p>
          Below are a few examples showing the input images and their corresponding classifications:
        </p>
        <div class="row">
          <div class="col-md-4 text-center">
            <img src="resources/images/Healthy.png" class="img-fluid">
            <p>Classification: Healthy</p>
          </div>
          <div class="col-md-4 text-center">
            <img src="resources/images/Powdery.png" class="img-fluid">
            <p>Classification: Powdery</p>
          </div>
          <div class="col-md-4 text-center">
            <img src="resources/images/Rust.png" class="img-fluid">
            <p>Classification: Rust</p>
          </div>
        </div>
      </div>
    </div>

    <!-- State of the Art -->
    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">State of the Art</h2>
        <p>
          Below are some state-of-the-art models used for plant disease detection along with their test accuracies:
        </p>

        <ul>
          <li><a href="https://www.kaggle.com/code/mahmoudweso/plant-disease-recognition/notebook" target="_blank">CNN Model</a> – Test Accuracy: 97.3%</li>
          <li><a href="https://www.kaggle.com/code/chanchal24/plant-disease-recognition-using-dl/notebook" target="_blank">CNN Model</a> – Test Accuracy: 88.3%</li>
          <li><a href="https://www.mdpi.com/2223-7747/9/10/1302" target="_blank">ResNet101 with FPN & ASPP</a> – Test Accuracy: 97.86%</li>
          <li><a href="https://www.mdpi.com/2223-7747/9/10/1302" target="_blank">ResNet-50 with SVM</a> – Test Accuracy: 97.86%</li>
          <li><a href="https://www.mdpi.com/2223-7747/9/10/1302" target="_blank">YOLO Model</a> – Test Accuracy: 94.58%</li>
        </ul>
      </div>
    </div>


    <!-- Project Updates Section -->
        <div class="row">
          <div class="col-lg-12 text-left">
            <h2 class="mt-5">Project Updates</h2>
    

        <h5 class="mt-5">Update #1: Updated the Baseline Model</h5>
        <p class="section-content">
           We started with our baseline model (see the image below) which started with a very simple CNN architecture. We ran and tested 
           this model on our data and it achieved an accuracy of 86.67%. 
        </p>

        <div class="img-container text-center">
          <img src="resources/images/model_baseline.png" class="img-fluid" alt="Update 1 Image" style="width: 7.69cm; height: 13.03cm;">
        </div>

        <h5 class="mt-5">Update #2: Tested More Complex CNN Models</h5>
        <p class="section-content">
          We explored a deeper CNN architecture to improve performance, where we added more layers and deepened the model which achieved an 
          accuracy of 91.67%. Then we implemented a GPU-powered CNN, enabling faster training times and improved experimentation with larger datasets and deeper models,
          which contained This is a convolutional neural network (CNN) with 11 layers (including padding, convolutional, batch normalization, ReLU, max pooling, flattening, and fully connected layers), designed for 3-class image classification with an input of 3-channel images which was larger than the previous models. This model got an accuracy of 95.5% and a very respectable performance
          in respect to its training and validation accuracy over the epochs (see picture below). Yet, ultimately we decided to explore a 
          different architecture as this model was too simple.

          <div class="img-container text-center">
            <img src="resources/images/CNN1.png" class="img-fluid" alt="Update 1 Image" style="width:13.03cm ; height: 7.69cm;">
          </div>
        </p>

        <h5 class="mt-5">Turning Point</h5>
        <p class="section-content">
          We decided that the CNN models being used were too trivial and basic and hence decided to revisit the literature review and explore more 
          complex models to use. The next sections will show what models we used, the hyperparameter tuning of the models (if any), and the updates we did to our preproccessing steps.
          The state-of-the-art included a model called YOLO which we attempted but were unsuccessful because the yolo model requires a very specific structure for its 
          input data and due to the time constraint we could not implement.
        </p>



        <h5 class="mt-5">Update #3: LSTM</h5>
        <p class="section-content">
          This LSTM-based model architecture is designed to handle sequential data by reshaping the input images into a format suitable for 
          LSTM layers. The model consists of three stacked LSTM layers with 128 units each, which are effective for capturing temporal 
          dependencies and spatial relationships in image sequences. The fully connected layers, along with the dropout regularization, 
          help to prevent overfitting, while the output layer with a softmax activation is used to classify the input into three categories
          making it suitable for our plant disease classification task. The model architecture is summarized in the picture below and it got and accuracy 
          of 66.64%. Furthermore, the same model was run over 50 epochs utlizing GPU and achieved an accuracy of 84.37% but was still not sufficient enough to be in consideration for a final model. 
          <div class="img-container text-center">
            <img src="resources/images/LSTM.png" class="img-fluid" alt="Update 1 Image" style="width: 7.69cm; height: 13.03cm;">
          </div>
          <div class="img-container text-center">
            <img src="resources/images/LSTM1.png" class="img-fluid" alt="Update 1 Image" style="width:13.03cm ; height: 7.69cm;">
          </div>
        </p>

        <h5 class="mt-5">Update #4: RNN</h5>
        <p class="section-content">
          This RNN-based model architecture is designed to process sequential data by reshaping the input images to fit the requirements 
          of the RNN layers. It consists of three stacked SimpleRNN layers with 128 units each, which helps capture temporal dependencies 
          in image sequences. The fully connected layers, combined with dropout regularization, prevent overfitting, while the output layer 
          with a softmax activation ensures the classification of images into three categories. This architecture is well-suited for 
          classifying plant diseases based on sequential features in images. The model architecture is summarized in the picture below and it got and accuracy 
          of 60.11 % which is very low and hence dropped from consideration when choosing a final model.
          <div class="img-container text-center">
            <img src="resources/images/RNN.png" class="img-fluid" alt="Update 1 Image" style="width: 7.69cm; height: 13.03cm;">
          </div>
          <div class="img-container text-center">
            <img src="resources/images/RNN1.png" class="img-fluid" alt="Update 1 Image" style="width:13.03cm ; height: 7.69cm;">
          </div>
        </p>

        <div class="row">
          <div class="col-lg-12 text-left section-margin">
            <h5 class="mt-5">Update #5: ResNet-34</h5>
        
            <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
              <span style="font-weight: bold; font-size: 1.1rem; color: #212529;">Key Components of ResNet-34</span>
              <br>
              The architecture starts with an initial convolutional layer with a 7x7 kernel size and a stride of 2. This layer is followed by batch normalization and a max-pooling layer, which serves to reduce the spatial resolution of the input image and extract the first set of low-level features. 
            </p>
        
            <ul style="line-height: 1.8; font-size: 1rem; padding-left: 20px;">
              <li>The first stage contains 3 residual blocks.</li>
              <li>The second stage contains 4 residual blocks.</li>
              <li>The third stage contains 6 residual blocks.</li>
              <li>The fourth stage contains 3 residual blocks.</li>
            </ul>
        
            <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
              These stages use filter sizes of 64, 128, 256, and 512, enabling the model to learn more abstract representations of the input image as it passes through the network. Each residual block within these stages contains two convolutional layers, followed by batch normalization, ReLU activations, and the crucial skip connection.
            </p>       
            <h3 style="margin-top: 30px; font-weight: bold; font-size: 1.1rem; color: #343a40;">Performance and Results</h3>
            <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
              Our implementation of the ResNet-34 model achieved an accuracy of <strong>95%</strong>, which is a very promising result. However, as shown below, the model appears to be overfitting, which is indicated by the gap between training and validation performance.
            </p>
        
            <div class="img-container text-center" style="margin: 30px auto;">
              <img src="resources/images/Resnet_result.png" class="img-fluid" alt="ResNet-34 Performance" style="width:13.03cm ; height: 7.69cm; border: 1px solid #dee2e6; border-radius: 5px; padding: 10px; background-color: #f8f9fa;">
            </div>
        
          </div>
        </div>
        
          <div class="row">
            <div class="col-lg-12 text-left section-margin">
              <h5 class="mt-5">Update #6: Inception V3</h5>
          
              <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
                <span style="font-weight: bold; font-size: 1.1rem; color: #212529;">Model Architecture</span>
                <br>
                The architecture of InceptionV3 begins with a base model that has been pre-trained on ImageNet, but with the top layer removed. This is achieved by setting include_top=False, ensuring the model outputs feature maps rather than final class predictions. These feature maps contain rich representations of the input image, capturing complex patterns that can be adapted for the new task of plant disease detection. The images are resized to (256, 256, 3) to match the input requirements of InceptionV3.
              </p>
          
              <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
                <span style="font-weight: bold; font-size: 1.1rem; color: #212529;">Fine-tuning the Model</span>
                <br>
                To tailor the model for plant disease detection, we unfreeze the top layers of the base InceptionV3 model, allowing them to be trainable. Specifically, the top 30 layers are made trainable, while the earlier layers remain frozen. This approach helps retain the knowledge learned from ImageNet, particularly the low-level visual features captured by the early layers. By freezing the early layers, we avoid overfitting and speed up training, as fewer parameters need to be updated.
              </p>
          
              <p style="background-color: #f8f9fa; padding: 20px; border-radius: 5px; font-size: 1rem; line-height: 1.8;">
                <span style="font-weight: bold; font-size: 1.1rem; color: #212529;">Additional Layers</span>
                <br>
                After the base model, a Global Average Pooling layer is added. This layer reduces the spatial dimensions of the feature maps, transforming them into a fixed-length feature vector. This operation not only reduces the number of parameters but also ensures that the model retains the most significant information from the feature maps. The feature vector is then passed through a fully connected Dense Layer with 1024 units and ReLU activation. To prevent overfitting, a Dropout Layer is included, with a rate of 0.6, randomly dropping some units during training to improve the model’s generalization ability. The final layer of the model is a Softmax Output Layer with 3 units, corresponding to the three classes in the plant disease detection task.
              </p>
          
              <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
                <span style="font-weight: bold; font-size: 1.1rem; color: #212529;">Training and Fine-tuning</span>
                <br>
                Fine-tuning is done by using a low learning rate of 1e-4, allowing the model to make small adjustments to the pre-trained weights. This ensures that the previously learned features are preserved while still adapting to the new task. The model is compiled using the Adam Optimizer with categorical cross-entropy as the loss function, which is ideal for multi-class classification problems. In addition, precision and recall metrics are used to monitor the model's performance, focusing on both false positives and false negatives.
              </p>
          
              <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
                <span style="font-weight: bold; font-size: 1.1rem; color: #212529;">Callbacks and Regularization</span>
                <br>
                To improve training efficiency and prevent overfitting, we use two important callbacks:
                <br>
                1. <strong>Early Stopping:</strong> This callback stops training when the validation loss has not improved for 10 consecutive epochs and restores the best weights from earlier in the training process.
                <br>
                2. <strong>Reduce Learning Rate:</strong> This callback reduces the learning rate by a factor of 0.2 when the validation loss plateaus, allowing the model to converge more quickly during the later stages of training.
              </p>
          
              <h3 style="margin-top: 30px; font-weight: bold; font-size: 1.1rem; color: #343a40;">Results</h3>
              <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
                The fine-tuned InceptionV3 model achieved outstanding results, reaching a top validation accuracy of 98.3%, surpassing the performance of other state-of-the-art models tested. Yet, it is important to note that the InceptionV3 model extremely underfits and hence even though it reached an extremely high accuracy, it would not be robust to unseen data.
              </p>
          
              <div class="img-container text-center">
                <img src="resources/images/inception.png" class="img-fluid" alt="Inception V3 Model Architecture" style="width:13.03cm ; height: 7.69cm;">
              </div>
            
              <div class="img-container text-center">
                <img src="resources/images/Inception_res.png" class="img-fluid" alt="Inception V3 Results" style="width: 10.08cm; height: 8.07cm;">
              </div>
            </div>
          </div>


        

        <div class="row">
          <div class="col-lg-12 text-left section-margin">
            <h5 class="mt-5">Update #7: Revisiting Preprocessing Steps</h5>
        
            <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
              <span style="font-weight: bold; font-size: 1.1rem; color: #212529;">Rescaling of Images</span>
              <br>
              The images are rescaled by dividing each pixel value by 255, which normalizes the pixel intensity values to the range [0, 1]. This helps improve the convergence speed of gradient-based optimization algorithms like stochastic gradient descent (SGD). The rescaling ensures consistency across both the training and validation datasets.
            </p>
        
            <p style="background-color: #f8f9fa; padding: 20px; border-radius: 5px; font-size: 1rem; line-height: 1.8;">
              <span style="font-weight: bold; font-size: 1.1rem; color: #212529;">Data Augmentation</span>
              <br>
              Data augmentation is applied to artificially expand the size of the training dataset by generating transformed versions of the original images. This increases the robustness of the model by exposing it to a broader variety of data. The following augmentation techniques are used:
            </p>
        
            <table class="table table-bordered" style="margin-top: 20px;">
              <thead>
                <tr style="background-color: #f1f3f5; text-align: center;">
                  <th style="font-size: 1.1rem;">Technique</th>
                  <th style="font-size: 1.1rem;">Description</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="padding: 15px; font-size: 1rem;">Rotation Range (30 degrees)</td>
                  <td style="padding: 15px; font-size: 1rem;">Images are randomly rotated within a 30-degree range to simulate various object orientations.</td>
                </tr>
                <tr>
                  <td style="padding: 15px; font-size: 1rem;">Width & Height Shifts (0.2)</td>
                  <td style="padding: 15px; font-size: 1rem;">The images are shifted horizontally and vertically by up to 20% of the total image width/height.</td>
                </tr>
                <tr>
                  <td style="padding: 15px; font-size: 1rem;">Shear Range (0.2)</td>
                  <td style="padding: 15px; font-size: 1rem;">Shearing transformations are applied, simulating perspective changes by slanting the images.</td>
                </tr>
                <tr>
                  <td style="padding: 15px; font-size: 1rem;">Zoom Range (0.2)</td>
                  <td style="padding: 15px; font-size: 1rem;">Random zooming is applied to simulate changes in focal length and object distance.</td>
                </tr>
                <tr>
                  <td style="padding: 15px; font-size: 1rem;">Horizontal Flip</td>
                  <td style="padding: 15px; font-size: 1rem;">Images are flipped horizontally to simulate different orientations.</td>
                </tr>
                <tr>
                  <td style="padding: 15px; font-size: 1rem;">Fill Mode</td>
                  <td style="padding: 15px; font-size: 1rem;">The <code>fill_mode='nearest'</code> ensures that empty pixels after transformations are filled with the nearest valid pixel value.</td>
                </tr>
              </tbody>
            </table>
        
            <p style="line-height: 1.8; font-size: 1.1rem; color: #495057; margin-top: 20px;">
              These transformations are applied using the <code>ImageDataGenerator</code> class from the <code>tensorflow.keras.preprocessing.image</code> module. The augmented data helps improve model generalization and avoids overfitting.
            </p>
        
            <h3 style="margin-top: 30px; font-weight: bold; font-size: 1.1rem; color: #343a40;">Results of Augmentation</h3>
            <p style="line-height: 1.8; font-size: 1.1rem; color: #495057;">
              We combined the augmented data with the original dataset and observed improved performance in model robustness. These preprocessing steps were critical in enabling our best models to achieve higher accuracy and better generalization.
              Furthermore, we optimized the hyperparameters of our models by decreasing the learning rate, adding dropout regularization and adam optimizers.
            </p>
          </div>
        </div>
    <div class="row">
      <div class="row">
        <div class="col-lg-12 text-left section-margin">
          <h2 class="mt-5 section-title">Results</h2>
      
          <p class="section-content">
            Our best performing models were the GPU-powered CNN and the ResNet-34, yet the accuracy and loss are not the only measures that need to be highlighted
            when choosing a final model to deploy. Firstly, let's compare the final results of both the CNN and ResNet-34 models after all said improvements.
          </p>
      
          <br/>
      
          <div class="table-responsive">
            <table class="table table-bordered text-center">
              <thead>
                <tr>
                  <th></th>
                  <th>CNN</th>
                  <th>ResNet-34</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Before Updates</strong></td>
                  <td>
                    <img src="resources/images/CNN12.jpg" class="img-fluid" alt="CNN Before Updates" style="max-width: 200px;">
                  </td>
                  <td>
                    <img src="resources/images/Resnet_result.png" class="img-fluid" alt="ResNet-34 Before Updates" style="max-width: 200px;">
                  </td>
                </tr>
                <tr>
                  <td><strong>After Updates</strong></td>
                  <td>
                    <p>Accuracy: 95.5%</p>
                    <img src="resources/images/CNN1.png" class="img-fluid" alt="CNN After Updates" style="max-width: 200px;">
                  </td>
                  <td>
                    <p>Accuracy: 96.6%</p>
                    <img src="resources/images/Resnew.png" class="img-fluid" alt="ResNet-34 After Updates" style="max-width: 200px;">
                  </td>
                </tr>
                <tr>
                  <td><strong>Advantages</strong></td>
                  <td>Simpler architecture, faster training</td>
                  <td>Higher accuracy, robust feature extraction</td>
                </tr>
                <tr>
                  <td><strong>Disadvantages</strong></td>
                  <td>Lower generalization, limited scalability</td>
                  <td>More computational resources required</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>
      
      <div class="row">
        <div class="col-lg-12 text-left">
          <h2 class="mt-5">Conclusion</h2>
          <p>
            As can be observed in the graphs in the previous section, the InceptionV3 model was extremely underfitting, yet the CNN and 
            the ResNet-34 were a little bit overfitting, but we were able to improve their performance and reduce the overfitting. 
            Hence, our best performing model was in fact the GPU powered CNN, even though it holds the simplest architecture, it is 
            the most efficient in terms of balancing time complexity and accuracy. Which is why we decided it to be our final model.

          </p>
        </div>
      </div>
  </div>
<!-- Bootstrap core JavaScript -->
<script src="../vendor/jquery/jquery.slim.min.js"></script>
<script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

</body>

</html>
